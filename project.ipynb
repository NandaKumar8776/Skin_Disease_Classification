{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Constants\n",
    "TARGET_SIZE = (180, 180)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data(root_dir):\n",
    "\n",
    "    # Lists to store data\n",
    "    all_images = []\n",
    "    y = []\n",
    "    full_path_all = []\n",
    "\n",
    "    # Define a mapping from folder name to label\n",
    "    disease_to_label = {\n",
    "        \"Eczema\": 0,\n",
    "        \"Melanoma\": 1,\n",
    "        \"Atopic_Dermatitis\": 2,\n",
    "        \"Basal_Cell_Carcinoma\": 3,\n",
    "        \"Melanocytic_Nevi\": 4,\n",
    "        \"Benign_Keratosis_like_Lesions\": 5,\n",
    "        \"Psoriasis\": 6,\n",
    "        \"Seborrheic_Keratoses\": 7,\n",
    "        \"Tinea\": 8,\n",
    "        \"Warts\": 9\n",
    "    }\n",
    "\n",
    "    # Function to process a directory\n",
    "    def process_directory(directory):\n",
    "        for item in os.listdir(directory):\n",
    "            full_path = os.path.join(directory, item)\n",
    "\n",
    "            # If it's a directory, process it recursively\n",
    "            if os.path.isdir(full_path):\n",
    "                process_directory(full_path)\n",
    "\n",
    "            # If it's an image file, add it to our lists\n",
    "            elif item.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                all_images.append(item)\n",
    "\n",
    "                # Assign a label based on the folder name\n",
    "                for disease, label in disease_to_label.items():\n",
    "                    if disease in directory:\n",
    "                        y.append(label)\n",
    "                        break\n",
    "\n",
    "                # Add full path of the image to the list\n",
    "                full_path_all.append(full_path)\n",
    "\n",
    "    # Start processing from root directory\n",
    "    process_directory(root_dir)\n",
    "\n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'image': all_images,\n",
    "        'y': y,\n",
    "        'full_path': full_path_all\n",
    "    })\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image filtering functions\n",
    "def resize_image(image_array):\n",
    "    \"\"\"Resize image to target size\"\"\"\n",
    "    return cv2.resize(image_array, TARGET_SIZE)\n",
    "\n",
    "def gaussian_blur_image(image_array):\n",
    "    \"\"\"Apply Gaussian blur to reduce noise\"\"\"\n",
    "    return cv2.GaussianBlur(image_array, (5, 5), 0)\n",
    "\n",
    "def denoise_image(image_array):\n",
    "    \"\"\"Apply fast non-local means denoising\"\"\"\n",
    "    return cv2.fastNlMeansDenoisingColored(image_array, None, 0.1, 0.1, 7, 21)\n",
    "\n",
    "def sharpen_image(image_array):\n",
    "    \"\"\"Sharpen image using a custom kernel\"\"\"\n",
    "    sharpening_kernel = np.array([[0, -1, 0],\n",
    "                                   [-1, 5, -1],\n",
    "                                   [0, -1, 0]])\n",
    "    return cv2.filter2D(image_array, -2, sharpening_kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image Augmentation functions\n",
    "def horizontal_flip_image(image_array):\n",
    "    \"\"\"Flip image horizontally\"\"\"\n",
    "    h_flipped_img = cv2.flip(image_array, 1)\n",
    "    return cv2.resize(h_flipped_img, TARGET_SIZE)\n",
    "\n",
    "def vertical_flip_image(image_array):\n",
    "    \"\"\"Flip image vertically\"\"\"\n",
    "    v_flipped_img = cv2.flip(image_array, 0)\n",
    "    return cv2.resize(v_flipped_img, TARGET_SIZE)\n",
    "\n",
    "def rotate_image(image_array, degree=cv2.ROTATE_90_CLOCKWISE):\n",
    "    \"\"\"Rotate image 90 degrees\"\"\"\n",
    "    rotated = cv2.rotate(image_array, degree)\n",
    "    return cv2.resize(rotated, TARGET_SIZE)\n",
    "\n",
    "def crop_image(image_array, crop_center_percent=0.6):\n",
    "    \"\"\"Crop center portion of the image\"\"\"\n",
    "    h, w, c = image_array.shape\n",
    "    crop_size = int(min(h, w) * crop_center_percent)  \n",
    "    start_x = (w - crop_size) // 2\n",
    "    start_y = (h - crop_size) // 2\n",
    "    cropped = image_array[start_y:start_y + crop_size, start_x:start_x + crop_size]\n",
    "    return cv2.resize(cropped, TARGET_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_and_augment_image(image_path):\n",
    "    \"\"\"\n",
    "    Apply filtering and augmentation to an image\n",
    "    \n",
    "    Args:\n",
    "        image_path (str): Path to the image file\n",
    "    \n",
    "    Returns:\n",
    "        list: List of augmented images\n",
    "    \"\"\"\n",
    "    # Open image and convert to numpy array\n",
    "    image = cv2.imread(image_path)\n",
    "    \n",
    "    # Resizing image\n",
    "    image = resize_image(image)\n",
    "\n",
    "    # Apply filtering techniques\n",
    "    image = denoise_image(image)\n",
    "    image = gaussian_blur_image(image)\n",
    "    image = sharpen_image(image)\n",
    "\n",
    "    # Perform augmentations\n",
    "    augmented_images = [\n",
    "        horizontal_flip_image(image),\n",
    "        vertical_flip_image(image),\n",
    "        rotate_image(image),\n",
    "        crop_image(image)\n",
    "    ]\n",
    "\n",
    "    return augmented_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_dataset(df, output_dir='augmented_images'):\n",
    "    \"\"\"\n",
    "    Augment the entire dataset by generating additional images\n",
    "    \n",
    "    Args:\n",
    "        df (pandas.DataFrame): Original dataset\n",
    "        output_dir (str, optional): Directory to save augmented images\n",
    "    \n",
    "    Returns:\n",
    "        pandas.DataFrame: Augmented dataset\n",
    "    \"\"\"\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Create lists to store augmented data\n",
    "    augmented_images = []\n",
    "    augmented_labels = []\n",
    "    augmented_full_paths = []\n",
    "\n",
    "    # Iterate through original dataset and apply augmentations\n",
    "    for index, row in df.iterrows():\n",
    "        # Add original image first\n",
    "        augmented_images.append(row['image'])\n",
    "        augmented_labels.append(row['y'])\n",
    "        augmented_full_paths.append(row['full_path'])\n",
    "\n",
    "        # Generate augmented images\n",
    "        aug_imgs = filter_and_augment_image(row['full_path'])\n",
    "        \n",
    "        # Add augmented images\n",
    "        for aug_img_idx, aug_img in enumerate(aug_imgs):\n",
    "            # Create unique filename for augmented image\n",
    "            aug_img_filename = f\"augmented_{os.path.splitext(row['image'])[0]}_{aug_img_idx}.jpg\"\n",
    "            aug_img_path = os.path.join(output_dir, aug_img_filename)\n",
    "            \n",
    "            # Save augmented image (will overwrite if exists)\n",
    "            cv2.imwrite(aug_img_path, aug_img)\n",
    "            \n",
    "            # Add to lists\n",
    "            augmented_images.append(aug_img_filename)\n",
    "            augmented_labels.append(row['y'])\n",
    "            augmented_full_paths.append(aug_img_path)\n",
    "\n",
    "    # Create new augmented DataFrame\n",
    "    augmented_df = pd.DataFrame({\n",
    "        'image': augmented_images,\n",
    "        'y': augmented_labels,\n",
    "        'full_path': augmented_full_paths\n",
    "    })\n",
    "\n",
    "    return augmented_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN Model Architecture\n",
    "class SkinDiseaseClassifier(nn.Module):\n",
    "    def __init__(self, num_classes=10):  \n",
    "        super(SkinDiseaseClassifier, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(64 * 45 * 45, 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes) \n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # The input is an image of size 3×180×180 (channels, width, height).\n",
    "        x = self.conv1(x)\n",
    "        # Output: 32×180×180 (3 -> 32 since 32 output channels; 180 is unchanged due to padding)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        # Output: 32x90x90 (since maxpooling of 2x2 with stride 2 reduces heigh x width by 2)\n",
    "        x = self.conv2(x)\n",
    "        # Output: 64×180×180 (32 -> 64 since 64 output channels; 180 is unchanged due to padding)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        # Output: 64x45x45 (since maxpooling of 2x2 with stride 2 reduces heigh x width by 2)\n",
    "        x = x.view(-1, 64 * 45 * 45)\n",
    "        # Flattened the image to a 1D array where 64*45*45 is the number of pixels (64- channels, 45 is width and height)\n",
    "        # Output: 1 x 129600\n",
    "        x = self.fc1(x)\n",
    "        # Takes the 129600 input and maps to 128 vectors (fully connected layers, given as per in the __init__) \n",
    "        # Output: 1 x 128\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        # Randomly sets 50% of the weights in the vectors, to 0 to prevent overfitting and make model more generalized by forcing other 50% of perceptrons to learn (as in __init__)\n",
    "        x = self.fc2(x)\n",
    "        # Final fully connected layer; where 128 vectors are fully connected to just 10 perceptrons (the number of classes)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset Class\n",
    "class SkinDiseaseDataset(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.image_paths = df['full_path'].tolist()\n",
    "        self.labels = df['y'].tolist()\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = Image.open(self.image_paths[index]).convert(\"RGB\")\n",
    "        label = self.labels[index]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 10986\n",
      "Validation set size: 3662\n",
      "Test set size: 3663\n"
     ]
    }
   ],
   "source": [
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(12)\n",
    "np.random.seed(12)\n",
    "\n",
    "# Load dataset\n",
    "#dataset_path = r'C:\\Users\\Nkay\\Desktop\\College\\Semester 2\\Deep Learning with Pytorch\\Project\\Dataset'\n",
    "dataset_path = r'D:\\College\\Deep learning with pytorch\\Dataset'\n",
    "data = create_data(dataset_path)\n",
    "\n",
    "# Augment dataset\n",
    "#augmented_data = augment_dataset(data)\n",
    "\n",
    "# Split the augmented dataset\n",
    "train_df, val_test_df = train_test_split(data, test_size=0.4, random_state=12)\n",
    "val_df, test_df = train_test_split(val_test_df, test_size=0.5, random_state=12)\n",
    "\n",
    "# Print dataset sizes\n",
    "print(f\"Training set size: {len(train_df)}\")\n",
    "print(f\"Validation set size: {len(val_df)}\")\n",
    "print(f\"Test set size: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>y</th>\n",
       "      <th>full_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0_1.jpg</td>\n",
       "      <td>2</td>\n",
       "      <td>D:\\College\\Deep learning with pytorch\\Dataset\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0_16.jpg</td>\n",
       "      <td>2</td>\n",
       "      <td>D:\\College\\Deep learning with pytorch\\Dataset\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0_17.jpg</td>\n",
       "      <td>2</td>\n",
       "      <td>D:\\College\\Deep learning with pytorch\\Dataset\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0_18.jpg</td>\n",
       "      <td>2</td>\n",
       "      <td>D:\\College\\Deep learning with pytorch\\Dataset\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0_19.jpg</td>\n",
       "      <td>2</td>\n",
       "      <td>D:\\College\\Deep learning with pytorch\\Dataset\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18306</th>\n",
       "      <td>v-warts-plantar-108.jpg</td>\n",
       "      <td>9</td>\n",
       "      <td>D:\\College\\Deep learning with pytorch\\Dataset\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18307</th>\n",
       "      <td>v-warts-plantar-23.jpg</td>\n",
       "      <td>9</td>\n",
       "      <td>D:\\College\\Deep learning with pytorch\\Dataset\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18308</th>\n",
       "      <td>v-warts-plantar-27.jpg</td>\n",
       "      <td>9</td>\n",
       "      <td>D:\\College\\Deep learning with pytorch\\Dataset\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18309</th>\n",
       "      <td>v-warts-plantar-67.jpg</td>\n",
       "      <td>9</td>\n",
       "      <td>D:\\College\\Deep learning with pytorch\\Dataset\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18310</th>\n",
       "      <td>v-warts-treatment-2.jpg</td>\n",
       "      <td>9</td>\n",
       "      <td>D:\\College\\Deep learning with pytorch\\Dataset\\...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18311 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         image  y  \\\n",
       "0                      0_1.jpg  2   \n",
       "1                     0_16.jpg  2   \n",
       "2                     0_17.jpg  2   \n",
       "3                     0_18.jpg  2   \n",
       "4                     0_19.jpg  2   \n",
       "...                        ... ..   \n",
       "18306  v-warts-plantar-108.jpg  9   \n",
       "18307   v-warts-plantar-23.jpg  9   \n",
       "18308   v-warts-plantar-27.jpg  9   \n",
       "18309   v-warts-plantar-67.jpg  9   \n",
       "18310  v-warts-treatment-2.jpg  9   \n",
       "\n",
       "                                               full_path  \n",
       "0      D:\\College\\Deep learning with pytorch\\Dataset\\...  \n",
       "1      D:\\College\\Deep learning with pytorch\\Dataset\\...  \n",
       "2      D:\\College\\Deep learning with pytorch\\Dataset\\...  \n",
       "3      D:\\College\\Deep learning with pytorch\\Dataset\\...  \n",
       "4      D:\\College\\Deep learning with pytorch\\Dataset\\...  \n",
       "...                                                  ...  \n",
       "18306  D:\\College\\Deep learning with pytorch\\Dataset\\...  \n",
       "18307  D:\\College\\Deep learning with pytorch\\Dataset\\...  \n",
       "18308  D:\\College\\Deep learning with pytorch\\Dataset\\...  \n",
       "18309  D:\\College\\Deep learning with pytorch\\Dataset\\...  \n",
       "18310  D:\\College\\Deep learning with pytorch\\Dataset\\...  \n",
       "\n",
       "[18311 rows x 3 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((180, 180)),\n",
    "    transforms.ToTensor()\n",
    "    #transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = SkinDiseaseDataset(train_df, transform=transform)\n",
    "val_dataset = SkinDiseaseDataset(val_df, transform=transform)\n",
    "test_dataset = SkinDiseaseDataset(test_df, transform=transform)\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Epoch 1/3 ---\n",
      "Training...\n",
      "Batch 10/172, Loss: 1.7634\n",
      "Batch 20/172, Loss: 1.3466\n",
      "Batch 30/172, Loss: 1.5297\n",
      "Batch 40/172, Loss: 1.3842\n",
      "Batch 50/172, Loss: 1.2188\n",
      "Batch 60/172, Loss: 1.3710\n",
      "Batch 70/172, Loss: 1.1325\n",
      "Batch 80/172, Loss: 1.1844\n",
      "Batch 90/172, Loss: 1.0562\n",
      "Batch 100/172, Loss: 1.1491\n",
      "Batch 110/172, Loss: 1.1359\n",
      "Batch 120/172, Loss: 1.1750\n",
      "Batch 130/172, Loss: 1.1943\n",
      "Batch 140/172, Loss: 0.8405\n",
      "Batch 150/172, Loss: 1.1481\n",
      "Batch 160/172, Loss: 0.9458\n",
      "Batch 170/172, Loss: 1.2218\n",
      "Train Loss: 1.2937, Train Accuracy: 54.85%\n",
      "Validating...\n",
      "Val Loss: 1.0317, Val Accuracy: 65.18%\n",
      "\n",
      "--- Epoch 2/3 ---\n",
      "Training...\n",
      "Batch 10/172, Loss: 0.9890\n",
      "Batch 20/172, Loss: 0.7198\n",
      "Batch 30/172, Loss: 0.8309\n",
      "Batch 40/172, Loss: 0.7931\n",
      "Batch 50/172, Loss: 0.9461\n",
      "Batch 60/172, Loss: 0.9665\n",
      "Batch 70/172, Loss: 0.8319\n",
      "Batch 80/172, Loss: 0.9650\n",
      "Batch 90/172, Loss: 0.8318\n",
      "Batch 100/172, Loss: 0.8864\n",
      "Batch 110/172, Loss: 1.1765\n",
      "Batch 120/172, Loss: 1.0596\n",
      "Batch 130/172, Loss: 0.8944\n",
      "Batch 140/172, Loss: 0.7917\n",
      "Batch 150/172, Loss: 1.1074\n",
      "Batch 160/172, Loss: 0.8655\n",
      "Batch 170/172, Loss: 0.9990\n",
      "Train Loss: 0.9730, Train Accuracy: 65.34%\n",
      "Validating...\n",
      "Val Loss: 0.8653, Val Accuracy: 69.20%\n",
      "\n",
      "--- Epoch 3/3 ---\n",
      "Training...\n",
      "Batch 10/172, Loss: 0.9255\n",
      "Batch 20/172, Loss: 0.7992\n",
      "Batch 30/172, Loss: 0.6997\n",
      "Batch 40/172, Loss: 0.9392\n",
      "Batch 50/172, Loss: 0.8303\n",
      "Batch 60/172, Loss: 0.7613\n",
      "Batch 70/172, Loss: 0.7148\n",
      "Batch 80/172, Loss: 0.5633\n",
      "Batch 90/172, Loss: 0.8442\n",
      "Batch 100/172, Loss: 0.9210\n",
      "Batch 110/172, Loss: 0.9453\n",
      "Batch 120/172, Loss: 0.8908\n",
      "Batch 130/172, Loss: 0.8905\n",
      "Batch 140/172, Loss: 0.7226\n",
      "Batch 150/172, Loss: 0.9650\n",
      "Batch 160/172, Loss: 0.7858\n",
      "Batch 170/172, Loss: 1.0043\n",
      "Train Loss: 0.8691, Train Accuracy: 68.81%\n",
      "Validating...\n",
      "Val Loss: 0.7787, Val Accuracy: 73.24%\n",
      "\n",
      "Testing...\n",
      "Test Loss: 0.7656, Test Accuracy: 72.70%\n"
     ]
    }
   ],
   "source": [
    "# Initialize model\n",
    "num_classes = 10  \n",
    "\n",
    "model = SkinDiseaseClassifier(num_classes=num_classes).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Define scheduler\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'\\n--- Epoch {epoch + 1}/{num_epochs} ---')\n",
    "    \n",
    "    # Training phase\n",
    "    model.train()\n",
    "    train_loss, train_correct, train_total = 0.0, 0, 0\n",
    "    print('Training...')\n",
    "    for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        train_total += labels.size(0)\n",
    "        train_correct += (predicted == labels).sum().item()\n",
    "\n",
    "        # Print status for every 10 batches\n",
    "        if (batch_idx + 1) % 10 == 0:\n",
    "            print(f'Batch {batch_idx + 1}/{len(train_loader)}, Loss: {loss.item():.4f}')\n",
    "    \n",
    "    train_accuracy = 100 * train_correct / train_total\n",
    "    print(f'Train Loss: {train_loss / len(train_loader):.4f}, Train Accuracy: {train_accuracy:.2f}%')\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss, val_correct, val_total = 0.0, 0, 0\n",
    "    print('Validating...')\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            val_total += labels.size(0)\n",
    "            val_correct += (predicted == labels).sum().item()\n",
    "\n",
    "    val_accuracy = 100 * val_correct / val_total\n",
    "    print(f'Val Loss: {val_loss / len(val_loader):.4f}, Val Accuracy: {val_accuracy:.2f}%')\n",
    "\n",
    "    # Step the scheduler\n",
    "    scheduler.step()\n",
    "\n",
    "# Test phase\n",
    "print('\\nTesting...')\n",
    "test_loss, test_correct, test_total = 0.0, 0, 0\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss += loss.item()\n",
    "        \n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        test_total += labels.size(0)\n",
    "        test_correct += (predicted == labels).sum().item()\n",
    "\n",
    "test_accuracy = 100 * test_correct / test_total\n",
    "print(f'Test Loss: {test_loss / len(test_loader):.4f}, Test Accuracy: {test_accuracy:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\N30N\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Epoch 1/5 ---\n",
      "Training...\n",
      "Batch 10/172, Loss: 1.3756\n",
      "Batch 20/172, Loss: 1.2088\n",
      "Batch 30/172, Loss: 0.9752\n",
      "Batch 40/172, Loss: 1.2553\n",
      "Batch 50/172, Loss: 0.8946\n",
      "Batch 60/172, Loss: 1.0631\n",
      "Batch 70/172, Loss: 0.9039\n",
      "Batch 80/172, Loss: 0.7697\n",
      "Batch 90/172, Loss: 0.6641\n",
      "Batch 100/172, Loss: 0.9014\n",
      "Batch 110/172, Loss: 0.7229\n",
      "Batch 120/172, Loss: 0.6226\n",
      "Batch 130/172, Loss: 0.7271\n",
      "Batch 140/172, Loss: 0.7599\n",
      "Batch 150/172, Loss: 0.8027\n",
      "Batch 160/172, Loss: 0.7742\n",
      "Batch 170/172, Loss: 0.7476\n",
      "Train Loss: 0.8988, Train Accuracy: 69.87%\n",
      "Validating...\n",
      "Val Loss: 0.6533, Val Accuracy: 77.55%\n",
      "\n",
      "--- Epoch 2/5 ---\n",
      "Training...\n",
      "Batch 10/172, Loss: 0.6000\n",
      "Batch 20/172, Loss: 0.6994\n",
      "Batch 30/172, Loss: 0.5271\n",
      "Batch 40/172, Loss: 0.5140\n",
      "Batch 50/172, Loss: 0.4972\n",
      "Batch 60/172, Loss: 0.7823\n",
      "Batch 70/172, Loss: 0.7591\n",
      "Batch 80/172, Loss: 0.6563\n",
      "Batch 90/172, Loss: 0.7073\n",
      "Batch 100/172, Loss: 0.6455\n",
      "Batch 110/172, Loss: 0.5772\n",
      "Batch 120/172, Loss: 0.4131\n",
      "Batch 130/172, Loss: 0.6765\n",
      "Batch 140/172, Loss: 0.6413\n",
      "Batch 150/172, Loss: 0.5588\n",
      "Batch 160/172, Loss: 0.5728\n",
      "Batch 170/172, Loss: 0.8318\n",
      "Train Loss: 0.6434, Train Accuracy: 77.45%\n",
      "Validating...\n",
      "Val Loss: 0.5862, Val Accuracy: 79.96%\n",
      "\n",
      "--- Epoch 3/5 ---\n",
      "Training...\n",
      "Batch 10/172, Loss: 0.4984\n",
      "Batch 20/172, Loss: 0.3075\n",
      "Batch 30/172, Loss: 0.6665\n",
      "Batch 40/172, Loss: 0.6668\n",
      "Batch 50/172, Loss: 0.5257\n",
      "Batch 60/172, Loss: 0.3917\n",
      "Batch 70/172, Loss: 0.6987\n",
      "Batch 80/172, Loss: 0.7240\n",
      "Batch 90/172, Loss: 0.7661\n",
      "Batch 100/172, Loss: 0.7944\n",
      "Batch 110/172, Loss: 0.6204\n",
      "Batch 120/172, Loss: 0.5123\n",
      "Batch 130/172, Loss: 0.5261\n",
      "Batch 140/172, Loss: 0.6808\n",
      "Batch 150/172, Loss: 0.5374\n",
      "Batch 160/172, Loss: 0.5409\n",
      "Batch 170/172, Loss: 0.5461\n",
      "Train Loss: 0.5888, Train Accuracy: 79.23%\n",
      "Validating...\n",
      "Val Loss: 0.5821, Val Accuracy: 79.57%\n",
      "\n",
      "--- Epoch 4/5 ---\n",
      "Training...\n",
      "Batch 10/172, Loss: 0.5600\n",
      "Batch 20/172, Loss: 0.4555\n",
      "Batch 30/172, Loss: 0.6171\n",
      "Batch 40/172, Loss: 0.6704\n",
      "Batch 50/172, Loss: 0.5452\n",
      "Batch 60/172, Loss: 0.6157\n",
      "Batch 70/172, Loss: 0.5235\n",
      "Batch 80/172, Loss: 0.6065\n",
      "Batch 90/172, Loss: 0.4593\n",
      "Batch 100/172, Loss: 0.5517\n",
      "Batch 110/172, Loss: 0.5322\n",
      "Batch 120/172, Loss: 0.5634\n",
      "Batch 130/172, Loss: 0.4690\n",
      "Batch 140/172, Loss: 0.6229\n",
      "Batch 150/172, Loss: 0.3482\n",
      "Batch 160/172, Loss: 0.4648\n",
      "Batch 170/172, Loss: 0.5447\n",
      "Train Loss: 0.5499, Train Accuracy: 80.04%\n",
      "Validating...\n",
      "Val Loss: 0.5598, Val Accuracy: 80.45%\n",
      "\n",
      "--- Epoch 5/5 ---\n",
      "Training...\n",
      "Batch 10/172, Loss: 0.6000\n",
      "Batch 20/172, Loss: 0.5377\n",
      "Batch 30/172, Loss: 0.3492\n",
      "Batch 40/172, Loss: 0.5713\n",
      "Batch 50/172, Loss: 0.5701\n",
      "Batch 60/172, Loss: 0.6819\n",
      "Batch 70/172, Loss: 0.5042\n",
      "Batch 80/172, Loss: 0.5562\n",
      "Batch 90/172, Loss: 0.4331\n",
      "Batch 100/172, Loss: 0.5967\n",
      "Batch 110/172, Loss: 0.4753\n",
      "Batch 120/172, Loss: 0.3028\n",
      "Batch 130/172, Loss: 0.2868\n",
      "Batch 140/172, Loss: 0.4834\n",
      "Batch 150/172, Loss: 0.5894\n",
      "Batch 160/172, Loss: 0.5975\n",
      "Batch 170/172, Loss: 0.4703\n",
      "Train Loss: 0.5187, Train Accuracy: 81.13%\n",
      "Validating...\n",
      "Val Loss: 0.5350, Val Accuracy: 81.43%\n",
      "\n",
      "Testing...\n",
      "Test Loss: 0.5474, Test Accuracy: 79.61%\n"
     ]
    }
   ],
   "source": [
    "### Using the pre-trained Resnet50 model\n",
    "\n",
    "import torchvision.models as models\n",
    "\n",
    "# Load pre-trained ResNet50 model\n",
    "model = models.resnet50(weights=True)\n",
    "\n",
    "# Freeze all layers for now\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False  \n",
    "\n",
    "# Replace the last fully connected layer with num_ftrs to 10 (10 classes) This is the only tunable layer now.\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, 10)  \n",
    "\n",
    "# Move the model to the appropriate device if exists\n",
    "model = model.to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Define learning rate scheduler\n",
    "#scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'\\n--- Epoch {epoch + 1}/{num_epochs} ---')\n",
    "    \n",
    "    # Training phase\n",
    "    model.train()\n",
    "    train_loss, train_correct, train_total = 0.0, 0, 0\n",
    "    print('Training...')\n",
    "    for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        train_total += labels.size(0)\n",
    "        train_correct += (predicted == labels).sum().item()\n",
    "\n",
    "        # Print status for every 10 batches\n",
    "        if (batch_idx + 1) % 10 == 0:\n",
    "            print(f'Batch {batch_idx + 1}/{len(train_loader)}, Loss: {loss.item():.4f}')\n",
    "    \n",
    "    train_accuracy = 100 * train_correct / train_total\n",
    "    print(f'Train Loss: {train_loss / len(train_loader):.4f}, Train Accuracy: {train_accuracy:.2f}%')\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss, val_correct, val_total = 0.0, 0, 0\n",
    "    print('Validating...')\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            val_total += labels.size(0)\n",
    "            val_correct += (predicted == labels).sum().item()\n",
    "\n",
    "    val_accuracy = 100 * val_correct / val_total\n",
    "    print(f'Val Loss: {val_loss / len(val_loader):.4f}, Val Accuracy: {val_accuracy:.2f}%')\n",
    "\n",
    "    # Step the scheduler\n",
    "    scheduler.step()\n",
    "\n",
    "# Test phase\n",
    "print('\\nTesting...')\n",
    "test_loss, test_correct, test_total = 0.0, 0, 0\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss += loss.item()\n",
    "        \n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        test_total += labels.size(0)\n",
    "        test_correct += (predicted == labels).sum().item()\n",
    "\n",
    "test_accuracy = 100 * test_correct / test_total\n",
    "print(f'Test Loss: {test_loss / len(test_loader):.4f}, Test Accuracy: {test_accuracy:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
